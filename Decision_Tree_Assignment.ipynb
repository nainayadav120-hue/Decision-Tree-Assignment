{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree Assignment"
      ],
      "metadata": {
        "id": "qcSA7-3Lc8jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "\"\"\" Decision Tree in Classification:A Decision Tree is a supervised learning algorithm used for both classification and regression tasks.\n",
        "In the context of classification, a Decision Tree:\n",
        "- Splits data into subsets based on feature values.\n",
        "- Creates a tree-like structure where each internal node represents a decision (or test) on a feature, each branch represents the outcome of the test, and each leaf node represents a class label.\n",
        "- Predicts the class of a new instance by starting at the root, following the tree based on feature values, and ending at a leaf node.\n",
        "\n",
        "How it works:\n",
        "1. Root node: The algorithm chooses the best feature to split the data (using metrics like Gini impurity or entropy/information gain).\n",
        "2. Splitting: Data is split into subsets based on feature values.\n",
        "3. Recursion: The process repeats for each subset until a stopping criterion is met (like max depth or all instances having the same class).\n",
        "4. Prediction: New instances follow the tree's decisions to reach a leaf node for the predicted class.\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "2MwiCMk-dI5T",
        "outputId": "889d5719-dce3-4ea0-98a4-e6ec7c982005"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Decision Tree in Classification:A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. \\nIn the context of classification, a Decision Tree:\\n- Splits data into subsets based on feature values.\\n- Creates a tree-like structure where each internal node represents a decision (or test) on a feature, each branch represents the outcome of the test, and each leaf node represents a class label.\\n- Predicts the class of a new instance by starting at the root, following the tree based on feature values, and ending at a leaf node.\\n\\nHow it works:\\n1. Root node: The algorithm chooses the best feature to split the data (using metrics like Gini impurity or entropy/information gain).\\n2. Splitting: Data is split into subsets based on feature values.\\n3. Recursion: The process repeats for each subset until a stopping criterion is met (like max depth or all instances having the same class).\\n4. Prediction: New instances follow the tree's decisions to reach a leaf node for the predicted class.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\"\"\" Impurity Measures in Decision Trees:Decision Trees use impurity measures to determine the best feature to split the data at each node.\n",
        "Two common impurity measures are Gini Impurity and Entropy.\n",
        "\n",
        "Gini Impurity:- Definition: Measures the probability of incorrectly classifying a randomly chosen element in the node if it were randomly labeled according to the class distribution of the node.\n",
        "- Formula: Gini Impurity = 1 - Σ (p_i)^2, where p_i is the proportion of instances of class i in the node.\n",
        "- Range: 0 (pure node) to 1 (impure node).\n",
        "\n",
        "Entropy:- Definition: Measures the uncertainty or disorder in the node. It quantifies the amount of information needed to specify the class of an instance in the node.\n",
        "- Formula: Entropy = - Σ (p_i * log2(p_i)), where p_i is the proportion of instances of class i in the node.\n",
        "- Range: 0 (pure node) to log2(k) (impure node), where k is the number of classes.\n",
        "\n",
        "Impact on Splits:Both Gini Impurity and Entropy guide the Decision Tree algorithm to choose the best splits:\n",
        "- Lower impurity (Gini or Entropy) after a split indicates a better separation of classes.\n",
        "- The algorithm selects the feature and split point that maximizes the reduction in impurity, resulting in more homogeneous child nodes.\n",
        "- Gini Impurity tends to favor larger partitions, while Entropy can be more sensitive to class distributions.\n",
        "\n",
        "Choosing between Gini and Entropy:- Gini Impurity is computationally more efficient and often performs well.\n",
        "- Entropy can lead to more balanced trees but might be computationally more expensive.\n",
        "- In practice, both measures often yield similar results, and the choice may depend on the specific problem or computational constraints.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "IADCp2MleDiX",
        "outputId": "7ff19700-7541-43b4-df83-088835ffd6c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Impurity Measures in Decision Trees:Decision Trees use impurity measures to determine the best feature to split the data at each node.\\nTwo common impurity measures are Gini Impurity and Entropy.\\n\\nGini Impurity:- Definition: Measures the probability of incorrectly classifying a randomly chosen element in the node if it were randomly labeled according to the class distribution of the node.\\n- Formula: Gini Impurity = 1 - Σ (p_i)^2, where p_i is the proportion of instances of class i in the node.\\n- Range: 0 (pure node) to 1 (impure node).\\n\\nEntropy:- Definition: Measures the uncertainty or disorder in the node. It quantifies the amount of information needed to specify the class of an instance in the node.\\n- Formula: Entropy = - Σ (p_i * log2(p_i)), where p_i is the proportion of instances of class i in the node.\\n- Range: 0 (pure node) to log2(k) (impure node), where k is the number of classes.\\n\\nImpact on Splits:Both Gini Impurity and Entropy guide the Decision Tree algorithm to choose the best splits:\\n- Lower impurity (Gini or Entropy) after a split indicates a better separation of classes.\\n- The algorithm selects the feature and split point that maximizes the reduction in impurity, resulting in more homogeneous child nodes.\\n- Gini Impurity tends to favor larger partitions, while Entropy can be more sensitive to class distributions.\\n\\nChoosing between Gini and Entropy:- Gini Impurity is computationally more efficient and often performs well.\\n- Entropy can lead to more balanced trees but might be computationally more expensive.\\n- In practice, both measures often yield similar results, and the choice may depend on the specific problem or computational constraints.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\"\"\" Pre-Pruning vs. Post-Pruning in Decision Trees:\n",
        "Pre-Pruning and Post-Pruning are techniques used to prevent overfitting in Decision Trees by controlling the complexity of the tree.\n",
        "\n",
        "Pre-Pruning:- Definition:\n",
        "Stops the tree growth early by specifying a stopping criterion (e.g., max depth, min samples per node) before the tree is fully grown.\n",
        "- Practical Advantage: Reduces computational cost and prevents overfitting by limiting tree complexity early.\n",
        "\n",
        "Post-Pruning:- Definition: Builds a full tree and then removes branches that provide little power to classify instances (using a pruning criterion like minimum cost complexity).\n",
        "- Practical Advantage: Can lead to a more optimal tree structure by considering the full tree before pruning, potentially improving accuracy.\n",
        "\n",
        "Key Differences:- Timing: Pre-Pruning occurs during tree construction, while Post-Pruning occurs after the tree is fully built.\n",
        "- Complexity: Pre-Pruning might underfit if the stopping criterion is too strict, while Post-Pruning can be more flexible but computationally expensive.\n",
        "- Performance: Post-Pruning often results in better performance due to its ability to assess the full tree structure, but it can be more resource-intensive.\n",
        "\n",
        "Choosing between Pre-Pruning and Post-Pruning:- Pre-Pruning is useful when computational resources are limited or when you want a quick, simpler model.\n",
        "- Post-Pruning is preferable when you want to maximize model performance and have sufficient computational resources.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "KhwdSLMEeDuf",
        "outputId": "7451aa19-8f77-4670-ee2d-7efb7431a177"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Pre-Pruning vs. Post-Pruning in Decision Trees:\\nPre-Pruning and Post-Pruning are techniques used to prevent overfitting in Decision Trees by controlling the complexity of the tree.\\n\\nPre-Pruning:- Definition:\\nStops the tree growth early by specifying a stopping criterion (e.g., max depth, min samples per node) before the tree is fully grown.\\n- Practical Advantage: Reduces computational cost and prevents overfitting by limiting tree complexity early.\\n\\nPost-Pruning:- Definition: Builds a full tree and then removes branches that provide little power to classify instances (using a pruning criterion like minimum cost complexity).\\n- Practical Advantage: Can lead to a more optimal tree structure by considering the full tree before pruning, potentially improving accuracy.\\n\\nKey Differences:- Timing: Pre-Pruning occurs during tree construction, while Post-Pruning occurs after the tree is fully built.\\n- Complexity: Pre-Pruning might underfit if the stopping criterion is too strict, while Post-Pruning can be more flexible but computationally expensive.\\n- Performance: Post-Pruning often results in better performance due to its ability to assess the full tree structure, but it can be more resource-intensive.\\n\\nChoosing between Pre-Pruning and Post-Pruning:- Pre-Pruning is useful when computational resources are limited or when you want a quick, simpler model.\\n- Post-Pruning is preferable when you want to maximize model performance and have sufficient computational resources.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\"\"\" Information Gain in Decision Trees:Information Gain measures the reduction in uncertainty or impurity in the data after splitting it on a particular feature. It's calculated as the difference between the impurity of the parent node and the weighted average impurity of the child nodes.\n",
        "\n",
        "Formula:Information Gain = Parent Node Impurity - (Weighted Average of Child Node Impurities)\n",
        "\n",
        "Importance for Choosing the Best Split:\n",
        "1. Maximizes Information: Information Gain helps identify the feature that provides the most information about the target variable.\n",
        "2. Reduces Uncertainty: By choosing the feature with the highest Information Gain, the Decision Tree algorithm reduces uncertainty in the data, leading to better splits.\n",
        "3. Improves Model Performance: By selecting the most informative features, Information Gain helps create a more accurate and efficient Decision Tree model.\n",
        "\n",
        "How it's Used:\n",
        "1. Calculate the impurity of the parent node (using metrics like Entropy or Gini Impurity).\n",
        "2. Calculate the impurity of each child node after splitting on a feature.\n",
        "3. Calculate the weighted average of child node impurities.\n",
        "4. Calculate Information Gain as the difference between parent node impurity and weighted average child node impurity.\n",
        "5. Choose the feature with the highest Information Gain for splitting.\n",
        "\n",
        "Benefits:\n",
        "1. Optimal Splits: Information Gain helps Decision Trees choose the most informative features, leading to optimal splits.\n",
        "2. Improved Accuracy: By reducing uncertainty and impurity, Information Gain contributes to better model performance.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "48U2veUCeD8N",
        "outputId": "5d73a7ea-47b2-4135-d78a-994dc626d69c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Information Gain in Decision Trees:Information Gain measures the reduction in uncertainty or impurity in the data after splitting it on a particular feature. It's calculated as the difference between the impurity of the parent node and the weighted average impurity of the child nodes.\\n\\nFormula:Information Gain = Parent Node Impurity - (Weighted Average of Child Node Impurities)\\n\\nImportance for Choosing the Best Split:\\n1. Maximizes Information: Information Gain helps identify the feature that provides the most information about the target variable.\\n2. Reduces Uncertainty: By choosing the feature with the highest Information Gain, the Decision Tree algorithm reduces uncertainty in the data, leading to better splits.\\n3. Improves Model Performance: By selecting the most informative features, Information Gain helps create a more accurate and efficient Decision Tree model.\\n\\nHow it's Used:\\n1. Calculate the impurity of the parent node (using metrics like Entropy or Gini Impurity).\\n2. Calculate the impurity of each child node after splitting on a feature.\\n3. Calculate the weighted average of child node impurities.\\n4. Calculate Information Gain as the difference between parent node impurity and weighted average child node impurity.\\n5. Choose the feature with the highest Information Gain for splitting.\\n\\nBenefits:\\n1. Optimal Splits: Information Gain helps Decision Trees choose the most informative features, leading to optimal splits.\\n2. Improved Accuracy: By reducing uncertainty and impurity, Information Gain contributes to better model performance.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\"\"\" Common Real-World Applications of Decision Trees:\n",
        "1. Credit Risk Assessment: Decision Trees are used to evaluate loan applicants' creditworthiness and predict the likelihood of default.\n",
        "2. Medical Diagnosis: Decision Trees help diagnose diseases based on symptoms, medical history, and test results.\n",
        "3. Customer Segmentation: Decision Trees are used to segment customers based on demographic and behavioral data, enabling targeted marketing and personalized sales approaches.\n",
        "4. Fraud Detection: Decision Trees help identify suspicious transactions and detect potential fraud in financial and insurance industries.\n",
        "5. Predictive Maintenance: Decision Trees predict equipment failures and maintenance needs in industries like manufacturing and energy.\n",
        "\n",
        "Main Advantages:\n",
        "1. Interpretability: Decision Trees are easy to understand and interpret, making them a popular choice for many applications.\n",
        "2. Handling Categorical Features: Decision Trees can handle categorical features directly, without requiring additional encoding.\n",
        "3. Non-Parametric: Decision Trees don't assume a specific distribution for the data, making them flexible and adaptable.\n",
        "4. Handling Missing Values: Decision Trees can handle missing values in the data, either by ignoring them or using surrogate splits.\n",
        "\n",
        "Limitations:\n",
        "1. Overfitting: Decision Trees can overfit the training data, especially when they are deep or complex.\n",
        "2. Instability: Small changes in the data can lead to large changes in the Decision Tree structure.\n",
        "3. Greedy Algorithm: Decision Trees use a greedy algorithm, which might not always find the optimal solution.\n",
        "4. Difficulty with Complex Relationships: Decision Trees can struggle to capture complex relationships between features, such as XOR or parity.\n",
        "\n",
        "Mitigating Limitations:\n",
        "1. Ensemble Methods: Techniques like Random Forests and Gradient Boosting can help mitigate overfitting and improve model performance.\n",
        "2. Pruning: Pruning techniques can help reduce overfitting and improve model interpretability.\n",
        "3. Hyperparameter Tuning: Careful tuning of hyperparameters, such as max depth and min samples per split, can help optimize Decision Tree performance.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "T2vj0lmTeERJ",
        "outputId": "07eaaf16-a8fe-4a5b-995c-992a58b6ba25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Common Real-World Applications of Decision Trees:\\n1. Credit Risk Assessment: Decision Trees are used to evaluate loan applicants' creditworthiness and predict the likelihood of default.\\n2. Medical Diagnosis: Decision Trees help diagnose diseases based on symptoms, medical history, and test results.\\n3. Customer Segmentation: Decision Trees are used to segment customers based on demographic and behavioral data, enabling targeted marketing and personalized sales approaches.\\n4. Fraud Detection: Decision Trees help identify suspicious transactions and detect potential fraud in financial and insurance industries.\\n5. Predictive Maintenance: Decision Trees predict equipment failures and maintenance needs in industries like manufacturing and energy.\\n\\nMain Advantages:\\n1. Interpretability: Decision Trees are easy to understand and interpret, making them a popular choice for many applications.\\n2. Handling Categorical Features: Decision Trees can handle categorical features directly, without requiring additional encoding.\\n3. Non-Parametric: Decision Trees don't assume a specific distribution for the data, making them flexible and adaptable.\\n4. Handling Missing Values: Decision Trees can handle missing values in the data, either by ignoring them or using surrogate splits.\\n\\nLimitations:\\n1. Overfitting: Decision Trees can overfit the training data, especially when they are deep or complex.\\n2. Instability: Small changes in the data can lead to large changes in the Decision Tree structure.\\n3. Greedy Algorithm: Decision Trees use a greedy algorithm, which might not always find the optimal solution.\\n4. Difficulty with Complex Relationships: Decision Trees can struggle to capture complex relationships between features, such as XOR or parity.\\n\\nMitigating Limitations:\\n1. Ensemble Methods: Techniques like Random Forests and Gradient Boosting can help mitigate overfitting and improve model performance.\\n2. Pruning: Pruning techniques can help reduce overfitting and improve model interpretability.\\n3. Hyperparameter Tuning: Careful tuning of hyperparameters, such as max depth and min samples per split, can help optimize Decision Tree performance.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion\n",
        "#● Print the model’s accuracy and feature importances\n",
        "#(Include your Python code and output in the code box below.)\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate model accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpWs1Tu9eEgi",
        "outputId": "a9932ac6-6475-436c-80d3-96c71be96066"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.00\n",
            "sepal width (cm): 0.02\n",
            "petal length (cm): 0.91\n",
            "petal width (cm): 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "#(Include your Python code and output in the code box below.)\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier with max_depth=3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_depth3:.2f}\")\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of fully-grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl37U9o6eEud",
        "outputId": "065cd75d-d2da-43e5-e613-3801b9a82fa1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy of fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to:\n",
        "#● Load the Boston Housing Dataset\n",
        "#● Train a Decision Tree Regressor\n",
        "#● Print the Mean Squared Error (MSE) and feature importances\n",
        "#(Include your Python code and output in the code box below.)\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate model performance\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(data.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHqbT6YBeE-H",
        "outputId": "81a734fe-c6ac-4afa-be60-85295fcfc87d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.50\n",
            "Feature Importances:\n",
            "MedInc: 0.53\n",
            "HouseAge: 0.05\n",
            "AveRooms: 0.05\n",
            "AveBedrms: 0.03\n",
            "Population: 0.03\n",
            "AveOccup: 0.13\n",
            "Latitude: 0.09\n",
            "Longitude: 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy\n",
        "#(Include your Python code and output in the code box below.)\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [1, 3, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Define the tune_decision_tree function\n",
        "def tune_decision_tree(X_train, y_train, param_grid):\n",
        "    dtree = DecisionTreeClassifier(random_state=42)\n",
        "    grid_search = GridSearchCV(dtree, param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search, grid_search.best_params_\n",
        "\n",
        "# Perform grid search\n",
        "grid_search, best_params = tune_decision_tree(X_train, y_train, param_grid)\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VcGh5QceFjX",
        "outputId": "b2711e0d-8be9-4604-98f6-39b893021879"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a\n",
        "#large dataset with mixed data types and some missing values.\n",
        "#Explain the step-by-step process you would follow to:\n",
        "#● Handle the missing values\n",
        "#● Encode the categorical features\n",
        "#● Train a Decision Tree model\n",
        "#● Tune its hyperparameters\n",
        "#● Evaluate its performance\n",
        "#And describe what business value this model could provide in the real-world setting.\n",
        "\"\"\" Step-by-Step Process for Predicting Disease Using Decision Trees\n",
        "1. Handle the Missing Values- Identify missing values: Use isnull() or isna() functions to detect missing values in the dataset.\n",
        "- Imputation methods: Choose an imputation method based on the data type and distribution:\n",
        "    - Mean/Median imputation for numerical features.\n",
        "    - Mode imputation for categorical features.\n",
        "    - Advanced imputation techniques like K-Nearest Neighbors (KNN) or Multiple Imputation by Chained Equations (MICE) can be used for more complex datasets.\n",
        "- Drop rows or columns: If the number of missing values is too high, consider dropping the rows or columns.\n",
        "\n",
        "2. Encode the Categorical Features- One-Hot Encoding (OHE): Use OHE to transform categorical features into numerical features. This method creates new columns for each category, with binary values indicating the presence or absence of the category.\n",
        "- Label Encoding: Use label encoding for ordinal categorical features, where categories have a natural order.\n",
        "\n",
        "3. Train a Decision Tree Model- Split data: Split the dataset into training and testing sets using train_test_split.\n",
        "- Train Decision Tree: Train a Decision Tree model using DecisionTreeClassifier from scikit-learn.\n",
        "\n",
        "4. Tune Hyperparameters- Define hyperparameter grid: Define a grid of hyperparameters to tune, including max_depth, min_samples_split, and min_samples_leaf.\n",
        "- Grid Search: Use GridSearchCV to perform a grid search over the hyperparameter grid, evaluating the model's performance using cross-validation.\n",
        "- Select best hyperparameters: Select the best hyperparameters based on the highest cross-validation accuracy.\n",
        "\n",
        "5. Evaluate Model Performance- Predict on test set: Use the trained model to predict on the test set.\n",
        "- Evaluate metrics: Evaluate the model's performance using metrics like accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "\n",
        "Business Value- Early disease detection:\n",
        "The model can help identify patients at high risk of developing the disease, enabling early intervention and treatment.\n",
        "- Personalized medicine: The model can help personalize treatment plans based on individual patient characteristics and disease risk.\n",
        "- Cost savings: By identifying high-risk patients and preventing disease progression, the model can help reduce healthcare costs.\n",
        "- Improved patient outcomes: The model can help improve patient outcomes by enabling early detection and treatment, reducing morbidity and mortality rates.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "PFgSn3KjeGk3",
        "outputId": "fab8edd0-c6b6-4ebc-aa09-231d613b757e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Step-by-Step Process for Predicting Disease Using Decision Trees\\n1. Handle the Missing Values- Identify missing values: Use isnull() or isna() functions to detect missing values in the dataset.\\n- Imputation methods: Choose an imputation method based on the data type and distribution:\\n    - Mean/Median imputation for numerical features.\\n    - Mode imputation for categorical features.\\n    - Advanced imputation techniques like K-Nearest Neighbors (KNN) or Multiple Imputation by Chained Equations (MICE) can be used for more complex datasets.\\n- Drop rows or columns: If the number of missing values is too high, consider dropping the rows or columns.\\n\\n2. Encode the Categorical Features- One-Hot Encoding (OHE): Use OHE to transform categorical features into numerical features. This method creates new columns for each category, with binary values indicating the presence or absence of the category.\\n- Label Encoding: Use label encoding for ordinal categorical features, where categories have a natural order.\\n\\n3. Train a Decision Tree Model- Split data: Split the dataset into training and testing sets using train_test_split.\\n- Train Decision Tree: Train a Decision Tree model using DecisionTreeClassifier from scikit-learn.\\n\\n4. Tune Hyperparameters- Define hyperparameter grid: Define a grid of hyperparameters to tune, including max_depth, min_samples_split, and min_samples_leaf.\\n- Grid Search: Use GridSearchCV to perform a grid search over the hyperparameter grid, evaluating the model's performance using cross-validation.\\n- Select best hyperparameters: Select the best hyperparameters based on the highest cross-validation accuracy.\\n\\n5. Evaluate Model Performance- Predict on test set: Use the trained model to predict on the test set.\\n- Evaluate metrics: Evaluate the model's performance using metrics like accuracy, precision, recall, F1-score, and ROC-AUC.\\n\\nBusiness Value- Early disease detection: \\nThe model can help identify patients at high risk of developing the disease, enabling early intervention and treatment.\\n- Personalized medicine: The model can help personalize treatment plans based on individual patient characteristics and disease risk.\\n- Cost savings: By identifying high-risk patients and preventing disease progression, the model can help reduce healthcare costs.\\n- Improved patient outcomes: The model can help improve patient outcomes by enabling early detection and treatment, reducing morbidity and mortality rates.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}